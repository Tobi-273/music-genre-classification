# -*- coding: utf-8 -*-
"""convolutional_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C2estNbdw5M6Bj7UriiI_qiKpHNo5QU-
"""

# batch size, epochs, loss, metrics, optimizer, model, steps_per_epoch, early stopping, dropout, regularization

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from pathlib import Path
from google.colab import drive

# for comparability of the different models
import numpy as np
np.random.seed(5)

drive.mount('/content/drive')

#train_dir = Path.cwd() / '/content/drive/MyDrive/AAA Private Ablage/Dateien/Studium/Leuphana/Python/genres/train' #Tobi
#test_dir = Path.cwd() / '/content/drive/MyDrive/AAA Private Ablage/Dateien/Studium/Leuphana/Python/genres/test' #Tobi

train_dir =Path.cwd() / '/content/drive/MyDrive/UNI/Machine Learning/genres/train' #jana
test_dir =Path.cwd() / '/content/drive/MyDrive/UNI/Machine Learning/genres/test' #jana

#train_dir = Path.cwd() / '/content/drive/MyDrive/Dies und Das/genres/train' #Sandra
#test_dir = Path.cwd() / '/content/drive/MyDrive/Dies und Das/genres/test' #Sandra

IMG_HEIGHT = 217
IMG_WIDTH = 334
epochs = 5
batch_size = 64

training_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)

train_data_gen = training_generator.flow_from_directory(
    directory=train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training')

validation_data_gen = training_generator.flow_from_directory(
    directory=train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation')

test_data_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(
    directory=test_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    class_mode='categorical')


# design the model
model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(16, (3, 3), activation=tf.nn.relu, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
                                    tf.keras.layers.MaxPooling2D((2, 2)),
                                    tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
                                    tf.keras.layers.MaxPooling2D((2, 2)),
                                    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
                                    tf.keras.layers.MaxPooling2D((2, 2)),

                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(2048, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# build the model
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])


history = model.fit(
    train_data_gen,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=validation_data_gen,
    steps_per_epoch=train_data_gen.samples // batch_size,
    validation_steps=validation_data_gen.samples // batch_size)

model.evaluate(test_data_gen)
model.summary()
model.save("saved_model")

# plot training and validation loss
loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs_loss = range(1, (epochs+1))
plt.plot(epochs_loss, loss_train, 'g', label='Training loss')
plt.plot(epochs_loss, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# plot training and validation accuracy
loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs_accuracy = range(1, (epochs+1))
plt.plot(epochs_accuracy, loss_train, 'g', label='Training accuracy')
plt.plot(epochs_accuracy, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Confusion Matrix
# function for plotting from https://colab.research.google.com/drive/1ISfhxFDntfOos7cOeT7swduSqzLEqyFn#scrollTo=z0DRPKl39290
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from collections import OrderedDict
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    """if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')"""

    print(cm)
    cm = np.asarray(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
        
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()


Y_pred = model.predict_generator(validation_data_gen, 120 // batch_size+1)
y_pred = np.argmax(Y_pred, axis=1)
print('Confusion Matrix')
cm = confusion_matrix(validation_data_gen.classes, y_pred)
print(cm)
print('Classification Report')
target_names = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']
print(classification_report(validation_data_gen.classes, y_pred, target_names=target_names))

genres = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}
keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()




plt.figure(figsize=(8,8))
plot_confusion_matrix(cm, keys, normalize=True)
plt.show()

"""Feature Maps"""

# using code from: https://medium.com/nerd-for-tech/visualizing-feature-maps-and-filters-eb05fb91d675

from tensorflow.keras.models import Model
import os
import cv2

inp= model.inputs 
print(inp)
out1= model.layers[0]. output  
print(out1)
feature_map_1= Model(inputs= inp, outputs= out1)  
feature_map_1.summary()

#finding out shape of filters for each layer
i=1
for layer in model.layers:
    if 'conv' in layer.name: 
        filters, bias= layer.get_weights()
        print('Filters Shape: '+ str(filters.shape, )+" " + 'Bias Shape: '+str(bias.shape)+ "<---- layer: "+str(i))
        print("-----------")
        i=i+1

#reading sample image, could also do this with folder_dir probably
os.chdir(r'/content/drive/MyDrive/UNI/Machine Learning/genres/train/jazz') #jana
img=plt.imread("jazz.00002.wav_mel.jpg")
plt.imshow(img)

#Feature Map Layer 1
img=plt.imread("jazz.00002.wav_mel.jpg")
input_img= np.expand_dims(img, axis=0) 
print(input_img.shape)                      # Printing out size of Input Image

f1=feature_map_1.predict(input_img)        # predicting out the Image 
print(f1.shape)                            
fig= plt.figure(figsize=(50,50))
for i in range(16):
    ax=fig.add_subplot(8,4,i+1)
    ax.imshow(f1[0,:,:,i])

#Feature Map Layer 2 (we could do this for all leayers in the CNN if we wanted to)

inp= model.inputs 

out2= model.layers[2]. output  # shape of the layer 2
feature_map_2= Model(inputs= inp, outputs= out2)  # submodel and pass input and output shapes


img=plt.imread("jazz.00002.wav_mel.jpg")   #sample image
input_img= np.expand_dims(img, axis=0)      
                      


f2=feature_map_2.predict(input_img)        # predicting out the Image 


fig= plt.figure(figsize=(50,50))
for i in range(32):
    ax=fig.add_subplot(8,8,i+1)
    ax.imshow(f2[0,:,:,i])

#filter weights of layer 1

layer= model.layers
layer_1= layer[0]
filter_1, bias_1= layer_1.get_weights()
print(filter_1.shape, bias_1.shape)

#Normalize the weights
f_min, f_max = filter_1.min(), filter_1.max()
filter_1 = (filter_1 - f_min) / (f_max - f_min)

fig= plt.figure(figsize=(50,50))
for i in range(16):
    ax = fig.add_subplot(8,4,i+1)
    ax.imshow(filter_1[:,:,:,i], cmap='gray')
